{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cDWOVZj9VdTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77326569-2b8a-40f4-86ff-e299e1f5a53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IU_NNd4KWYbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7651e6b8-991b-4fa1-adbb-a6340d7128a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "recipe-ingredients-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  recipe-ingredients-dataset.zip\n",
            "replace test.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: food-ingredients-and-recipe-dataset-with-images.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  food-ingredients-and-recipe-dataset-with-images.zip\n",
            "replace Food Images/Food Images/-bloody-mary-tomato-toast-with-celery-and-horseradish-56389813.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# get api key from kaggle settings, save kaggle.json to the files\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# recipe ingredients dataset to train standardization model\n",
        "!kaggle datasets download kaggle/recipe-ingredients-dataset\n",
        "!unzip recipe-ingredients-dataset.zip\n",
        "\n",
        "# dataset to be used on by model to get standardized recipe lists\n",
        "!kaggle datasets download pes12017000148/food-ingredients-and-recipe-dataset-with-images\n",
        "!unzip food-ingredients-and-recipe-dataset-with-images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NvPI80SxXPUC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TfPapEZPvZKF"
      },
      "outputs": [],
      "source": [
        "# open unzipped files\n",
        "f_test = open('test.json')\n",
        "f_train = open('train.json')\n",
        "data = json.load(f_train) + json.load(f_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu0oZ1tavyZY",
        "outputId": "931a84e7-78cf-4a04-df13-926d74d19afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of ingredients list: 7137\n",
            "first 10 ingredients: ['soba', 'low sodium pinto beans', 'turkey tenderloins', 'hungarian paprika', 'rice mix', 'crusty rolls', \"Piment d'Espelette\", 'greek style seasoning', 'condensed cream of mushroom soup', 'spices']\n"
          ]
        }
      ],
      "source": [
        "ingredients = set()\n",
        "\n",
        "# iterate through all ingredients in each recipe\n",
        "for recipe in data:\n",
        "  ings = [ing for ing in recipe['ingredients']]\n",
        "  for ing in ings:\n",
        "    # use set to avoid duplicate entries\n",
        "    ingredients.add(ing)\n",
        "\n",
        "# convert to list for easier operations\n",
        "ingredients = list(ingredients)\n",
        "\n",
        "print(f\"length of ingredients list: {len(ingredients)}\")\n",
        "print(f\"first 10 ingredients: {ingredients[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "m7J245-rAyRm"
      },
      "outputs": [],
      "source": [
        "# set of adjectives to look out for\n",
        "mods = set()\n",
        "mods1 = ['baked', 'blanched', 'blackened', 'braised', 'breaded', 'broiled', 'caramelized', 'charred', 'fermented', 'fried',\n",
        "         'glazed', 'infused', 'marinated', 'poached', 'roasted', 'sauteed', 'seared', 'smoked', 'whipped']\n",
        "mods2 = ['diced', 'battered', 'blackened', 'blanched', 'blended', 'boiled', 'boned', 'braised', 'brewed', 'broiled',\n",
        "           'browned', 'butterflied', 'candied', 'canned', 'caramelized', 'charred', 'chilled', 'chopped', 'clarified',\n",
        "           'condensed', 'creamed', 'crystalized', 'curdled', 'cured', 'curried', 'dehydrated', 'deviled', 'diluted',\n",
        "           'dredged', 'drenched', 'dried', 'drizzled', 'dry roasted', 'dusted', 'escalloped', 'evaporated', 'fermented',\n",
        "           'filled', 'folded', 'freeze dried', 'fricaseed', 'fried', 'glazed', 'granulated', 'grated', 'griddled', 'grilled',\n",
        "           'hardboiled', 'homogenized', 'kneaded', 'malted', 'mashed', 'minced', 'mixed', 'medium', 'small', 'large',\n",
        "           'packed', 'pan-fried', 'parboiled', 'parched', 'pasteurized', 'peppered', 'pickled', 'powdered', 'preserved',\n",
        "           'pulverized', 'pureed', 'redolent', 'reduced', 'refrigerated', 'chilled', 'roasted', 'rolled', 'salted',\n",
        "           'saturated', 'scalded', 'scorched', 'scrambled', 'seared', 'seasoned', 'shredded', 'skimmed', 'sliced',\n",
        "           'slivered', 'smothered', 'soaked', 'soft-boiled', 'hard-boiled', 'stewed', 'stuffed', 'toasted', 'whipped',\n",
        "           'wilted', 'wrapped']\n",
        "# remove duplicates\n",
        "for adj_list in [mods1, mods2]:\n",
        "    for mod in adj_list:\n",
        "        mods.add(mod)\n",
        "\n",
        "# convert to list\n",
        "mods = list(mods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "L9whsRaTBUh_"
      },
      "outputs": [],
      "source": [
        "# set of units to look out for\n",
        "units = set()\n",
        "\n",
        "units1 = ['l', 'dl', 'milliliter', 'liter', 'deciliter', 'teaspoon', 't.', 'tsp.',\n",
        "             'milliliters', 'liters', 'deciliters', 'teaspoons', 't.', 'tsp.',\n",
        "            'tablespoon', 'T.', 'tbsp.', 'ounce', 'fl oz', 'cup', 'c.', 'pint', 'pt.',\n",
        "            'tablespoons', 'ounces', 'fl ozs', 'cups', 'pints', 'quarts', 'gallons', 'grams', 'kilograms',\n",
        "            'quart', 'qt.', 'gallon', 'gal', 'mg', 'milligram', 'g', 'gram', 'kg', 'kilogram', 'milligrams',\n",
        "            'pound', 'lb', 'ounce', 'oz', 'count', 'pints', 'quarts', 'cups', 'tablespoons',\n",
        "            'pounds', 'lbs', 'ounces', 'units', 'drops', 'tsps.', 'tbsps.', 'Ts.', 'ts.',\n",
        "            'teaspoons', 'dash', 'pinch', 'drop', 'dram', 'smidgeon', 'dashes', 'pinches', 'drops',\n",
        "             'drams', 'smidgeons', ]\n",
        "\n",
        "for unit_list in [units1]:\n",
        "    for unit in unit_list:\n",
        "        units.add(unit)\n",
        "\n",
        "units = list(units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yik-vh-Bp5J",
        "outputId": "b466ef08-8f81-43e1-faef-c4205da7d58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n",
            "62\n"
          ]
        }
      ],
      "source": [
        "print(len(mods))\n",
        "print(len(units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9EeJu7IMCRd7"
      },
      "outputs": [],
      "source": [
        "# set of quantities to look out for, in both string and integer representations\n",
        "quantities = {\n",
        "    \"1/2\": 0.5,\n",
        "    \"1/4\": 0.25,\n",
        "    \"1/3\": 0.333,\n",
        "    \"2/3\": 0.666,\n",
        "    \"3/4\": 0.75,\n",
        "    \"half\": 0.5,\n",
        "    \"third\": 0.333,\n",
        "    \"quarter\": 0.25,\n",
        "    \"1\": 1,\n",
        "    \"2\": 2,\n",
        "    \"3\": 3,\n",
        "    \"4\": 4,\n",
        "    \"5\": 5,\n",
        "    \"6\": 6,\n",
        "    \"7\": 7,\n",
        "    \"8\": 8,\n",
        "    \"9\": 9,\n",
        "    \"10\": 10,\n",
        "    \"11\": 11,\n",
        "    \"12\": 12,\n",
        "    \"a dozen\": 12,\n",
        "    \"a baker's dozen\": 13,\n",
        "    \"two dozen\": 24,\n",
        "    \"three dozen\": 36,\n",
        "    \"one\": 1,\n",
        "    \"two\": 2,\n",
        "    \"three\": 3,\n",
        "    \"four\": 4,\n",
        "    \"five\": 5,\n",
        "    \"six\": 6,\n",
        "    \"seven\": 7,\n",
        "    \"eight\": 8,\n",
        "    \"nine\": 9,\n",
        "    \"ten\": 10,\n",
        "    \"eleven\": 11,\n",
        "    \"twelve\": 12\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "THRP3eHvLQJJ"
      },
      "outputs": [],
      "source": [
        "# build the training data\n",
        "# generate ingredient data\n",
        "def generate_data(num):\n",
        "  # lists of length num\n",
        "  X = [0] * num\n",
        "  Y = [0] * num\n",
        "\n",
        "  for i in range(num):\n",
        "    # pick random quantity from list (either string or integer form)\n",
        "    rnd_quantity_str, rnd_quantity_int = random.choice(list(quantities.items()))\n",
        "\n",
        "    # pick random unit\n",
        "    rnd_unit = random.choice(units)\n",
        "\n",
        "    # only pick random mod 1/3 of time\n",
        "    choose_mod = random.choice([None, None, True])\n",
        "\n",
        "    # pick random modifier if to be used (if selected true in statement above)\n",
        "    rnd_mod = random.choice(mods)\n",
        "\n",
        "    # pick random ingredient\n",
        "    rnd_ing = random.choice(ingredients)\n",
        "\n",
        "    # no unit 1/5 of time (ex: 5 pickles, chopped) use 'count' as placeholder\n",
        "    no_unit = random.choice([False, False, False, False, True])\n",
        "\n",
        "    if no_unit:\n",
        "      rnd_unit = 'count'\n",
        "\n",
        "    # output Y, words separated into dictionary\n",
        "    if choose_mod:\n",
        "      # ex: {\"qty\": 36, \"unit\": \"count\", \"item\": \"eggs\", \"mod\": \"scrambled\"}\n",
        "      Y[i] = f'{{ quantity: {rnd_quantity_int} , unit: {rnd_unit} , item: {rnd_ing} , mod: {rnd_mod} }}'\n",
        "    else:\n",
        "      # ex: \"mod\": None\n",
        "      Y[i] = f'{{ quantity: {rnd_quantity_int} , unit: {rnd_unit} , item: {rnd_ing} , mod: {None} }}'\n",
        "\n",
        "    # input X, in original form (ex: \"3 dozen scrambled eggs\")\n",
        "\n",
        "    # some ingredients have modifications at end (ex: 5 eggs, scrambled vs 3 dozen scrambled eggs)\n",
        "    # 1/3 of time there is a modification, 1/2 of time the modification is end (1/6)\n",
        "    rnd_end_mod = random.choice([False, True])\n",
        "\n",
        "    if choose_mod:\n",
        "      # no units\n",
        "      if no_unit:\n",
        "        # modification at end or in middle of phrase\n",
        "        if rnd_end_mod:\n",
        "          # ex: 3 eggs, scrambled\n",
        "          X[i] = f'{rnd_quantity_str} {rnd_ing} , {rnd_mod}'\n",
        "        else:\n",
        "          # ex: 3 scrambled eggs\n",
        "          X[i] = f'{rnd_quantity_str} {rnd_mod} {rnd_ing}'\n",
        "      else:\n",
        "        # add unit compared to previous\n",
        "        if rnd_end_mod:\n",
        "          # ex: 3 cups eggs, scrambled\n",
        "          X[i] = f'{rnd_quantity_str} {rnd_unit} {rnd_ing} , {rnd_mod}'\n",
        "        else:\n",
        "          # ex: 3 cups scrambled eggs\n",
        "          X[i] = f'{rnd_quantity_str} {rnd_unit} {rnd_mod} {rnd_ing}'\n",
        "    else:\n",
        "      # no modification\n",
        "      if no_unit:\n",
        "        # ex: 3 eggs\n",
        "        X[i] = f'{rnd_quantity_str} {rnd_ing}'\n",
        "      else:\n",
        "        # ex: 3 cups eggs\n",
        "        X[i] = f'{rnd_quantity_str} {rnd_unit} {rnd_ing}'\n",
        "\n",
        "  return ((X, Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzoJmgsqMFtc",
        "outputId": "94b1c96a-b2f4-45fd-b7e0-6f010a7af448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 pt. bottled peperoncini  ==>  { quantity: 9 , unit: pt. , item: bottled peperoncini , mod: None }\n",
            "1/3 dream whip  ==>  { quantity: 0.333 , unit: count , item: dream whip , mod: None }\n",
            "eight unsweetened iced tea  ==>  { quantity: 8 , unit: count , item: unsweetened iced tea , mod: None }\n",
            "third ounce shredded figs  ==>  { quantity: 0.333 , unit: ounce , item: figs , mod: shredded }\n",
            "7 liters italian salad dressing mix  ==>  { quantity: 7 , unit: liters , item: italian salad dressing mix , mod: None }\n",
            "2 fl ozs fish  ==>  { quantity: 2 , unit: fl ozs , item: fish , mod: None }\n",
            "three dozen milliliters preshred low fat mozzarella chees , caramelized  ==>  { quantity: 36 , unit: milliliters , item: preshred low fat mozzarella chees , mod: caramelized }\n",
            "eight dash black chickpeas , redolent  ==>  { quantity: 8 , unit: dash , item: black chickpeas , mod: redolent }\n",
            "2/3 smidgeon flavored wine , drenched  ==>  { quantity: 0.666 , unit: smidgeon , item: flavored wine , mod: drenched }\n",
            "half tbsps. kippered herring fillets  ==>  { quantity: 0.5 , unit: tbsps. , item: kippered herring fillets , mod: None }\n",
            "eleven fl oz Velveeta Cheese Spread  ==>  { quantity: 11 , unit: fl oz , item: Velveeta Cheese Spread , mod: None }\n",
            "quarter pints cremini caps  ==>  { quantity: 0.25 , unit: pints , item: cremini caps , mod: None }\n",
            "nine chopped celery  ==>  { quantity: 9 , unit: count , item: chopped celery , mod: None }\n",
            "five gal grated white tuna in water  ==>  { quantity: 5 , unit: gal , item: white tuna in water , mod: grated }\n",
            "12 pinches roasted hazelnuts  ==>  { quantity: 12 , unit: pinches , item: roasted hazelnuts , mod: None }\n"
          ]
        }
      ],
      "source": [
        "# run some examples\n",
        "N_TRAINING = 100_000\n",
        "\n",
        "x, y = generate_data(N_TRAINING)\n",
        "\n",
        "for i in range(15):\n",
        "    print(f\"{x[i]}  ==>  {y[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UIsLOisw0AJ0"
      },
      "outputs": [],
      "source": [
        "# MODEL BACKGROUND INFO by simon\n",
        "\n",
        "  # recurrent neural network\n",
        "    # different amounts of input data\n",
        "    # feedback loop allows using sequencial data (past data)\n",
        "    # input is both current and previous data\n",
        "    # problem: long sequences cause vanishing gradient descent (too long causes weights to become super big or small)\n",
        "\n",
        "  # LSTM (long short term memory)\n",
        "    # avoids exploding/vanishing gradient problem\n",
        "      # instead of using the same feedback loop, split into long term and short term memories\n",
        "    # forget gate: short term memory output determines what percent of long term memory is remembered (sigmoid)\n",
        "    # new long term memory is input to determine new short term memory to pass on\n",
        "\n",
        "  # word embedding and word2vec\n",
        "    # neural network assigns numbers to words based on context\n",
        "      #better than randomly assign as similar words have similar embeddings\n",
        "    # to include more context\n",
        "      # continuous bag of words: uses surrounding words to predict what goes in the middle\n",
        "      # skip gram: use middle word to predict surrounding words\n",
        "    # negative sampling: randomly selecting subset of words to not predict, optimizes training by reducing number of words\n",
        "\n",
        "   #seq2seq (uses all of the above)\n",
        "    # encoder: turns input into collection of long and short term memories (cell and hidden states)\n",
        "      # embedding layer to tokenize phrase into words and their embeddings\n",
        "      # layers of LSTM\n",
        "      # last cell and hidden states are called context vector\n",
        "    # decoder:\n",
        "      # input is context vector, decodes into output sentence\n",
        "      # new sets of LSTM\n",
        "      # lead to fully connected layer (basic neural network)\n",
        "        # softmax function picks output words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWKE70boxb4L",
        "outputId": "f436e740-b9de-4cac-bc37-72254379b5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7336 unique words used for training\n",
            "Longest number of words in input or output: 24\n",
            "['', '[UNK]', ',', 'dozen', 'two', 'three', 'a', 'four', '4', 'half', 'one', 'eight', 'seven', 'nine', '9', 'eleven', 'twelve', '2', '5', '1', 'six', 'quarter', '8', '6', '10']\n",
            "\n",
            "['', '[UNK]', ',', '}', '{', 'unit:', 'quantity:', 'mod:', 'item:', 'SOS', 'EOS', 'None', 'count', '12', '4', '9', '8', '1', '7', '3', '2', '6', '0.25', '0.5', '5']\n"
          ]
        }
      ],
      "source": [
        "# seq2seq model in this case\n",
        "# word tokenization\n",
        "print(f\"{len(ingredients) + len(mods) + len(units) + len(quantities)} unique words used for training\")\n",
        "# round up to 7400\n",
        "VOCAB_SIZE = 7400\n",
        "\n",
        "max_len = -1\n",
        "for i in range(N_TRAINING):\n",
        "    x_len = len(x[i].split())\n",
        "    y_len = len(y[i].split())\n",
        "    if x_len > max_len:\n",
        "        max_len = x_len\n",
        "    if y_len > max_len:\n",
        "        max_len = y_len\n",
        "\n",
        "print(f\"Longest number of words in input or output: {max_len}\")\n",
        "MAX_LENGTH = 30 # round up to be safe\n",
        "\n",
        "# text vectorization using keras functions\n",
        "# output_sequence_length are all same length by adding padding\n",
        "tv_layer_x = keras.layers.TextVectorization(VOCAB_SIZE, standardize=None, split='whitespace', output_sequence_length=MAX_LENGTH)\n",
        "tv_layer_y = keras.layers.TextVectorization(VOCAB_SIZE, standardize=None, split='whitespace', output_sequence_length=MAX_LENGTH)\n",
        "\n",
        "# use on training data\n",
        "tv_layer_x.adapt(x)\n",
        "# add start token and end token to data\n",
        "tv_layer_y.adapt([f\"SOS {seq} EOS\" for seq in y])\n",
        "\n",
        "print(tv_layer_x.get_vocabulary()[:25])\n",
        "print(\"\")\n",
        "print(tv_layer_y.get_vocabulary()[:25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRSqol5jwbaI",
        "outputId": "05db2337-86b3-4f65-f114-f6744e3b54e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input train shape: (90000,)\n",
            "decoder input train shape: (90000,)\n",
            "decoder targets train shape: (90000, 30)\n",
            "encoder input train example: b'9 pt. bottled peperoncini'\n",
            "decoder input train example: b'SOS { quantity: 9 , unit: pt. , item: bottled peperoncini , mod: None }'\n",
            "decoder target train example: [  4   6  15   2   5  89   2   8 532 860   2   7  11   3  10   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ],
      "source": [
        "# setting up inputs for model\n",
        "train_size = int(0.9 * N_TRAINING)\n",
        "\n",
        "encoder_input_train = tf.convert_to_tensor(x[:train_size])\n",
        "encoder_input_val = tf.convert_to_tensor(x[train_size:])\n",
        "\n",
        "# decoder input is what we want it to predict at the end of that timestep\n",
        "decoder_input_train = tf.convert_to_tensor([f\"SOS {seq}\" for seq in y[:train_size]])\n",
        "decoder_input_val = tf.convert_to_tensor([f\"SOS {seq}\" for seq in y[train_size:]])\n",
        "\n",
        "# use layer since need integer tokens instead of string\n",
        "decoder_targets_train = tv_layer_y([f\"{seq} EOS\" for seq in y[:train_size]])\n",
        "decoder_targets_val = tv_layer_y([f\"{seq} EOS\" for seq in y[train_size:]])\n",
        "\n",
        "print(f\"encoder input train shape: {encoder_input_train.shape}\")\n",
        "print(f\"decoder input train shape: {decoder_input_train.shape}\")\n",
        "print(f\"decoder targets train shape: {decoder_targets_train.shape}\")\n",
        "\n",
        "print(f\"encoder input train example: {encoder_input_train[0].numpy()}\")\n",
        "print(f\"decoder input train example: {decoder_input_train[0].numpy()}\")\n",
        "print(f\"decoder target train example: {decoder_targets_train[0].numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V45bJJTd00BI",
        "outputId": "0d47cdc2-59be-4ed4-c33a-1b15701a9f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input id shape: (None, 30)\n",
            "encoder input id type: <dtype: 'int64'>\n",
            "encoder embeddings shape: (None, 30, 96)\n",
            "decoder embeddings shape: (None, 30, 96)\n"
          ]
        }
      ],
      "source": [
        "# setting up seq2seq model\n",
        "# turn strings into numbers and add dimensionality for better representation as input to computer\n",
        "encoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
        "\n",
        "EMBED_SIZE = 96\n",
        "\n",
        "encoder_input_ids = tv_layer_x(encoder_inputs)\n",
        "decoder_input_ids = tv_layer_y(decoder_inputs)\n",
        "\n",
        "print(f\"encoder input id shape: {encoder_input_ids.shape}\")\n",
        "print(f\"encoder input id type: {encoder_input_ids.dtype}\")\n",
        "\n",
        "# input dim. x output dim. ==> (7000, 64)\n",
        "encoder_embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE, mask_zero=True)\n",
        "decoder_embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE, mask_zero=True)\n",
        "\n",
        "# keras functional api\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
        "\n",
        "print(f\"encoder embeddings shape: {encoder_embeddings.shape}\")\n",
        "print(f\"decoder embeddings shape: {decoder_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "qEMMbF4t1_2r"
      },
      "outputs": [],
      "source": [
        "# set up encoder\n",
        "encoder = keras.layers.Bidirectional(keras.layers.LSTM(256, return_state=True))\n",
        "\n",
        "# discard outputs, keep cell and hidden states\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "\n",
        "# short term memory is 0 and 2, long term memory is 1 and 3\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1), tf.concat(encoder_state[1::2], axis=-1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "W4KhlcrB25St"
      },
      "outputs": [],
      "source": [
        "# set up decoder\n",
        "# should be twice the size as encoder as bidirectional is concatenated (forward + backward = 2 * forward)\n",
        "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
        "\n",
        "# decoder input is encoder states\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "cBX1AS_-3Suu"
      },
      "outputs": [],
      "source": [
        "# set up dense layer to make predictions\n",
        "# softmax gives probability of all words, choose the highest as next word\n",
        "output_layer = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
        "Y_probabilities = output_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "g14UcrHx4WQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed1e9f4-fd39-41bc-cc54-1e6a3ff275a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " text_vectorization_2 (Text  (None, 30)                   0         ['input_5[0][0]']             \n",
            " Vectorization)                                                                                   \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)     (None, 30, 96)               710400    ['text_vectorization_2[1][0]']\n",
            "                                                                                                  \n",
            " text_vectorization_3 (Text  (None, 30)                   0         ['input_6[0][0]']             \n",
            " Vectorization)                                                                                   \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirecti  [(None, 512),                722944    ['embedding_4[0][0]']         \n",
            " onal)                        (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)     (None, 30, 96)               710400    ['text_vectorization_3[1][0]']\n",
            "                                                                                                  \n",
            " tf.concat_4 (TFOpLambda)    (None, 512)                  0         ['bidirectional_2[0][1]',     \n",
            "                                                                     'bidirectional_2[0][3]']     \n",
            "                                                                                                  \n",
            " tf.concat_5 (TFOpLambda)    (None, 512)                  0         ['bidirectional_2[0][2]',     \n",
            "                                                                     'bidirectional_2[0][4]']     \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)               (None, 30, 512)              1247232   ['embedding_5[0][0]',         \n",
            "                                                                     'tf.concat_4[0][0]',         \n",
            "                                                                     'tf.concat_5[0][0]']         \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 30, 7400)             3796200   ['lstm_5[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7187176 (27.42 MB)\n",
            "Trainable params: 7187176 (27.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# compile model\n",
        "model = keras.Model(inputs=[encoder_inputs,\n",
        "                    decoder_inputs],\n",
        "                    outputs=[Y_probabilities])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",  # sparse because each word token gets its own number (not one-hot)\n",
        "              optimizer=\"nadam\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "LxM-58El4mpd"
      },
      "outputs": [],
      "source": [
        "# turns the model.summary() into a picture\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# plot_model(model, to_file = 'model.png', show_shapes = True, show_dtype = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBoBH7m45Fdt",
        "outputId": "06245a17-7bbc-4fcc-db58-bd4f5540f4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11\n",
            "2813/2813 [==============================] - 136s 43ms/step - loss: 1.2977 - accuracy: 0.7612 - val_loss: 0.6577 - val_accuracy: 0.8647\n",
            "Epoch 2/11\n",
            "2813/2813 [==============================] - 104s 37ms/step - loss: 0.3851 - accuracy: 0.9177 - val_loss: 0.2210 - val_accuracy: 0.9501\n",
            "Epoch 3/11\n",
            "2813/2813 [==============================] - 104s 37ms/step - loss: 0.1301 - accuracy: 0.9691 - val_loss: 0.0873 - val_accuracy: 0.9772\n",
            "Epoch 4/11\n",
            "2813/2813 [==============================] - 105s 37ms/step - loss: 0.0536 - accuracy: 0.9854 - val_loss: 0.0333 - val_accuracy: 0.9920\n",
            "Epoch 5/11\n",
            "2813/2813 [==============================] - 104s 37ms/step - loss: 0.0185 - accuracy: 0.9962 - val_loss: 0.0144 - val_accuracy: 0.9971\n",
            "Epoch 6/11\n",
            "2813/2813 [==============================] - 105s 37ms/step - loss: 0.0095 - accuracy: 0.9981 - val_loss: 0.0082 - val_accuracy: 0.9985\n",
            "Epoch 7/11\n",
            "2813/2813 [==============================] - 104s 37ms/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.0096 - val_accuracy: 0.9976\n",
            "Epoch 8/11\n",
            "2813/2813 [==============================] - 104s 37ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.0071 - val_accuracy: 0.9983\n",
            "Epoch 9/11\n",
            "2813/2813 [==============================] - 104s 37ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0049 - val_accuracy: 0.9989\n",
            "Epoch 10/11\n",
            "2813/2813 [==============================] - 106s 38ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0058 - val_accuracy: 0.9986\n",
            "Epoch 11/11\n",
            "2813/2813 [==============================] - 117s 42ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9989\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7efb5c7b5240>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "EPOCHS = 11\n",
        "model.fit((encoder_input_train, decoder_input_train),\n",
        "          decoder_targets_train,\n",
        "          epochs = EPOCHS,\n",
        "          validation_data = ((encoder_input_val, decoder_input_val), decoder_targets_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ingredient string to json formatted dictionary representation\n",
        "def convert(ingredient):\n",
        "  # translation is blank at first\n",
        "  translation = \"\"\n",
        "  for word_id in range(MAX_LENGTH):\n",
        "    # string to be translated on encoder input\n",
        "    x_enc = np.array([ingredient])\n",
        "    # string to be translated on decoder input\n",
        "    x_dec = np.array([\"SOS \" + translation])\n",
        "    # softmax probability distribution for predicted next word\n",
        "    y_probs = model.predict((x_enc, x_dec), verbose = 0)[0, word_id]\n",
        "    # choose highest probability word\n",
        "    predicted_id = np.argmax(y_probs)\n",
        "    # find word based on id\n",
        "    predicted_word = tv_layer_y.get_vocabulary()[predicted_id]\n",
        "    # if word is EOS, means reached max length or end\n",
        "    if predicted_word == \"EOS\":\n",
        "      break\n",
        "    # else, add to prediction sequence and loop again for next word\n",
        "    translation += \" \" + predicted_word\n",
        "  return translation.strip()\n"
      ],
      "metadata": {
        "id": "53lyjxf2k0bz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Food Ingredients and Recipe Dataset with Image Name Mapping.csv\")\n",
        "df.head()\n",
        "# the images are in a separate folder\n",
        "# indexes should match to the recipes\n",
        "\n",
        "df.columns\n",
        "\n",
        "example = df.head()\n",
        "example\n",
        "\n",
        "import ast\n",
        "\n",
        "# contains list of standardized ingredients for each recipe\n",
        "standardized_column = []\n",
        "\n",
        "for recipe_ingredients in example['Ingredients'].items():\n",
        "  # list has quotes around it, need to convert from string to list\n",
        "  recipe_ingredients = ast.literal_eval(recipe_ingredients[1])\n",
        "  # adds recipe's standardized ingredients\n",
        "  standardized_ingredients = [];\n",
        "  for ingredient in recipe_ingredients:\n",
        "    if len(ingredient) != 0:\n",
        "      standardized_ingredients.append(convert(ingredient))\n",
        "  standardized_column.append(standardized_ingredients)\n",
        "  # reuse list for next recipe\n",
        "  standardized_ingredients.clear()\n",
        "\n",
        "# add column to df\n",
        "example.insert(6, \"Standardized Ingredients\", standardized_column, True)\n",
        "\n",
        "print(\"New dataframe with standardized ingredients column\")\n",
        "example\n",
        "example.columns\n",
        "# for recipe_ing in df['Ingredients'].items():\n",
        "  # list has quotes around it, need to convert from string to list\n",
        "  # recipe_ing = ast.literal_eval(recipe_ing[1])\n",
        "  # iterate through list\n",
        "  # for ing in recipe_ing:\n",
        "    # if len(ing) != 0:\n",
        "      # print(ing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "qvzwtZuljBYt",
        "outputId": "377edea6-7dd5-4d81-b3eb-f23d47ea7317"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fcef0a39fa48>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Food Ingredients and Recipe Dataset with Image Name Mapping.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# the images are in a separate folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# indexes should match to the recipes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbE8ZHRkOEFs",
        "outputId": "aa4e0009-c567-4e4c-edb6-6d6187ffbe8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mv6NKcBORqD",
        "outputId": "7791c6a3-b8a0-4606-a9ef-75d785db06b7"
      },
      "outputs": [],
      "source": [
        "# get api key from kaggle settings, save kaggle.json to the files\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# recipe ingredients dataset to train standardization model\n",
        "!kaggle datasets download kaggle/recipe-ingredients-dataset\n",
        "!unzip recipe-ingredients-dataset.zip\n",
        "\n",
        "# dataset to be used on by model to get standardized recipe lists\n",
        "!kaggle datasets download pes12017000148/food-ingredients-and-recipe-dataset-with-images\n",
        "!unzip food-ingredients-and-recipe-dataset-with-images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gvO_gdU5OThg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "op9vrjjvOWyj"
      },
      "outputs": [],
      "source": [
        "# open unzipped files\n",
        "f_test = open('test.json')\n",
        "f_train = open('train.json')\n",
        "data = json.load(f_train) + json.load(f_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pM-ElS9OYlh",
        "outputId": "b4de692f-293d-4f99-f9f8-80f3aacc8995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of ingredients list: 7137\n",
            "first 10 ingredients: ['salad oil', 'pork sausages', 'knorr cilantro minicub', 'Pace Salsa', 'chile powder', 'vanilla', 'sesame salt', 'horse gram', 'king salmon', 'greater galangal']\n"
          ]
        }
      ],
      "source": [
        "ingredients = set()\n",
        "\n",
        "# iterate through all ingredients in each recipe\n",
        "for recipe in data:\n",
        "  ings = [ing for ing in recipe['ingredients']]\n",
        "  for ing in ings:\n",
        "    # use set to avoid duplicate entries\n",
        "    ingredients.add(ing)\n",
        "\n",
        "# convert to list for easier operations\n",
        "ingredients = list(ingredients)\n",
        "\n",
        "print(f\"length of ingredients list: {len(ingredients)}\")\n",
        "print(f\"first 10 ingredients: {ingredients[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uvTPTCYrOa3n"
      },
      "outputs": [],
      "source": [
        "# set of adjectives to look out for\n",
        "mods = set()\n",
        "mods1 = ['baked', 'blanched', 'blackened', 'braised', 'breaded', 'broiled', 'caramelized', 'charred', 'fermented', 'fried',\n",
        "         'glazed', 'infused', 'marinated', 'poached', 'roasted', 'sauteed', 'seared', 'smoked', 'whipped']\n",
        "mods2 = ['diced', 'battered', 'blackened', 'blanched', 'blended', 'boiled', 'boned', 'braised', 'brewed', 'broiled',\n",
        "           'browned', 'butterflied', 'candied', 'canned', 'caramelized', 'charred', 'chilled', 'chopped', 'clarified',\n",
        "           'condensed', 'creamed', 'crystalized', 'curdled', 'cured', 'curried', 'dehydrated', 'deviled', 'diluted',\n",
        "           'dredged', 'drenched', 'dried', 'drizzled', 'dry roasted', 'dusted', 'escalloped', 'evaporated', 'fermented',\n",
        "           'filled', 'folded', 'freeze dried', 'fricaseed', 'fried', 'glazed', 'granulated', 'grated', 'griddled', 'grilled',\n",
        "           'hardboiled', 'homogenized', 'kneaded', 'malted', 'mashed', 'minced', 'mixed', 'medium', 'small', 'large',\n",
        "           'packed', 'pan-fried', 'parboiled', 'parched', 'pasteurized', 'peppered', 'pickled', 'powdered', 'preserved',\n",
        "           'pulverized', 'pureed', 'redolent', 'reduced', 'refrigerated', 'chilled', 'roasted', 'rolled', 'salted',\n",
        "           'saturated', 'scalded', 'scorched', 'scrambled', 'seared', 'seasoned', 'shredded', 'skimmed', 'sliced',\n",
        "           'slivered', 'smothered', 'soaked', 'soft-boiled', 'hard-boiled', 'stewed', 'stuffed', 'toasted', 'whipped',\n",
        "           'wilted', 'wrapped']\n",
        "# remove duplicates\n",
        "for adj_list in [mods1, mods2]:\n",
        "    for mod in adj_list:\n",
        "        mods.add(mod)\n",
        "\n",
        "# convert to list\n",
        "mods = list(mods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "40A5lnVyOdBv"
      },
      "outputs": [],
      "source": [
        "# set of units to look out for\n",
        "units = set()\n",
        "\n",
        "units1 = ['l', 'dl', 'milliliter', 'liter', 'deciliter', 'teaspoon', 't.', 'tsp.',\n",
        "             'milliliters', 'liters', 'deciliters', 'teaspoons', 't.', 'tsp.',\n",
        "            'tablespoon', 'T.', 'tbsp.', 'ounce', 'fl oz', 'cup', 'c.', 'pint', 'pt.',\n",
        "            'tablespoons', 'ounces', 'fl ozs', 'cups', 'pints', 'quarts', 'gallons', 'grams', 'kilograms',\n",
        "            'quart', 'qt.', 'gallon', 'gal', 'mg', 'milligram', 'g', 'gram', 'kg', 'kilogram', 'milligrams',\n",
        "            'pound', 'lb', 'ounce', 'oz', 'count', 'pints', 'quarts', 'cups', 'tablespoons',\n",
        "            'pounds', 'lbs', 'ounces', 'units', 'drops', 'tsps.', 'tbsps.', 'Ts.', 'ts.',\n",
        "            'teaspoons', 'dash', 'pinch', 'drop', 'dram', 'smidgeon', 'dashes', 'pinches', 'drops',\n",
        "             'drams', 'smidgeons', ]\n",
        "\n",
        "for unit_list in [units1]:\n",
        "    for unit in unit_list:\n",
        "        units.add(unit)\n",
        "\n",
        "units = list(units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgt9fL3AOexa",
        "outputId": "84eeeb3b-cf41-4dc1-fbe1-869eadd4da86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101\n",
            "62\n"
          ]
        }
      ],
      "source": [
        "print(len(mods))\n",
        "print(len(units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N5KXOOueOgmU"
      },
      "outputs": [],
      "source": [
        "# set of quantities to look out for, in both string and integer representations\n",
        "quantities = {\n",
        "    \"1/2\": 0.5,\n",
        "    \"1/4\": 0.25,\n",
        "    \"1/3\": 0.333,\n",
        "    \"2/3\": 0.666,\n",
        "    \"3/4\": 0.75,\n",
        "    \"half\": 0.5,\n",
        "    \"third\": 0.333,\n",
        "    \"quarter\": 0.25,\n",
        "    \"1\": 1,\n",
        "    \"2\": 2,\n",
        "    \"3\": 3,\n",
        "    \"4\": 4,\n",
        "    \"5\": 5,\n",
        "    \"6\": 6,\n",
        "    \"7\": 7,\n",
        "    \"8\": 8,\n",
        "    \"9\": 9,\n",
        "    \"10\": 10,\n",
        "    \"11\": 11,\n",
        "    \"12\": 12,\n",
        "    \"a dozen\": 12,\n",
        "    \"a baker's dozen\": 13,\n",
        "    \"two dozen\": 24,\n",
        "    \"three dozen\": 36,\n",
        "    \"one\": 1,\n",
        "    \"two\": 2,\n",
        "    \"three\": 3,\n",
        "    \"four\": 4,\n",
        "    \"five\": 5,\n",
        "    \"six\": 6,\n",
        "    \"seven\": 7,\n",
        "    \"eight\": 8,\n",
        "    \"nine\": 9,\n",
        "    \"ten\": 10,\n",
        "    \"eleven\": 11,\n",
        "    \"twelve\": 12\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OIIzeG2sOiR1"
      },
      "outputs": [],
      "source": [
        "# build the training data\n",
        "# generate ingredient data\n",
        "def generate_data(num):\n",
        "  # lists of length num\n",
        "  X = [0] * num\n",
        "  Y = [0] * num\n",
        "\n",
        "  for i in range(num):\n",
        "    # pick random quantity from list (either string or integer form)\n",
        "    rnd_quantity_str, rnd_quantity_int = random.choice(list(quantities.items()))\n",
        "\n",
        "    # pick random unit\n",
        "    rnd_unit = random.choice(units)\n",
        "\n",
        "    # only pick random mod 1/3 of time\n",
        "    choose_mod = random.choice([None, None, True])\n",
        "\n",
        "    # pick random modifier if to be used (if selected true in statement above)\n",
        "    rnd_mod = random.choice(mods)\n",
        "\n",
        "    # pick random ingredient\n",
        "    rnd_ing = random.choice(ingredients)\n",
        "\n",
        "    # no unit 1/5 of time (ex: 5 pickles, chopped) use 'count' as placeholder\n",
        "    no_unit = random.choice([False, False, False, False, True])\n",
        "\n",
        "    if no_unit:\n",
        "      rnd_unit = 'count'\n",
        "\n",
        "    # output Y, words separated into dictionary\n",
        "    if choose_mod:\n",
        "      # ex: {\"qty\": 36, \"unit\": \"count\", \"item\": \"eggs\", \"mod\": \"scrambled\"}\n",
        "      Y[i] = f'{{ quantity: {rnd_quantity_int} , unit: {rnd_unit} , item: {rnd_ing} , mod: {rnd_mod} }}'\n",
        "    else:\n",
        "      # ex: \"mod\": None\n",
        "      Y[i] = f'{{ quantity: {rnd_quantity_int} , unit: {rnd_unit} , item: {rnd_ing} , mod: {None} }}'\n",
        "\n",
        "    # input X, in original form (ex: \"3 dozen scrambled eggs\")\n",
        "\n",
        "    # some ingredients have modifications at end (ex: 5 eggs, scrambled vs 3 dozen scrambled eggs)\n",
        "    # 1/3 of time there is a modification, 1/2 of time the modification is end (1/6)\n",
        "    rnd_end_mod = random.choice([False, True])\n",
        "\n",
        "    if choose_mod:\n",
        "      # no units\n",
        "      if no_unit:\n",
        "        # modification at end or in middle of phrase\n",
        "        if rnd_end_mod:\n",
        "          # ex: 3 eggs, scrambled\n",
        "          X[i] = f'{rnd_quantity_str} {rnd_ing} , {rnd_mod}'\n",
        "        else:\n",
        "          # ex: 3 scrambled eggs\n",
        "          X[i] = f'{rnd_quantity_str} {rnd_mod} {rnd_ing}'\n",
        "      else:\n",
        "        # add unit compared to previous\n",
        "        if rnd_end_mod:\n",
        "          # ex: 3 cups eggs, scrambled\n",
        "          X[i] = f'{rnd_quantity_str} {rnd_unit} {rnd_ing} , {rnd_mod}'\n",
        "        else:\n",
        "          # ex: 3 cups scrambled eggs\n",
        "          X[i] = f'{rnd_quantity_str} {rnd_unit} {rnd_mod} {rnd_ing}'\n",
        "    else:\n",
        "      # no modification\n",
        "      if no_unit:\n",
        "        # ex: 3 eggs\n",
        "        X[i] = f'{rnd_quantity_str} {rnd_ing}'\n",
        "      else:\n",
        "        # ex: 3 cups eggs\n",
        "        X[i] = f'{rnd_quantity_str} {rnd_unit} {rnd_ing}'\n",
        "\n",
        "  return ((X, Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es20L5y9Oj74",
        "outputId": "d3d02d0f-d311-499e-96c1-4ac3b12253ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 quickcooking grits  ==>  { quantity: 10 , unit: count , item: quickcooking grits , mod: None }\n",
            "ten carrot sticks  ==>  { quantity: 10 , unit: count , item: carrot sticks , mod: None }\n",
            "third smidgeon cod fillets  ==>  { quantity: 0.333 , unit: smidgeon , item: cod fillets , mod: None }\n",
            "three dozen ounce corn syrup  ==>  { quantity: 36 , unit: ounce , item: corn syrup , mod: None }\n",
            "six quart pears  ==>  { quantity: 6 , unit: quart , item: pears , mod: None }\n",
            "third laurel leaves  ==>  { quantity: 0.333 , unit: count , item: laurel leaves , mod: None }\n",
            "half dashes Niçoise olives , candied  ==>  { quantity: 0.5 , unit: dashes , item: Niçoise olives , mod: candied }\n",
            "1/2 quart tabbouleh  ==>  { quantity: 0.5 , unit: quart , item: tabbouleh , mod: None }\n",
            "quarter drams frozen cod fillets  ==>  { quantity: 0.25 , unit: drams , item: frozen cod fillets , mod: None }\n",
            "2/3 lbs Kraft Miracle Whip Dressing  ==>  { quantity: 0.666 , unit: lbs , item: Kraft Miracle Whip Dressing , mod: None }\n",
            "twelve milliliter sandwich rolls  ==>  { quantity: 12 , unit: milliliter , item: sandwich rolls , mod: None }\n",
            "8 lbs white cannellini beans  ==>  { quantity: 8 , unit: lbs , item: white cannellini beans , mod: None }\n",
            "six ounces redolent shrimp paste  ==>  { quantity: 6 , unit: ounces , item: shrimp paste , mod: redolent }\n",
            "two lbs chicken breast strips  ==>  { quantity: 2 , unit: lbs , item: chicken breast strips , mod: None }\n",
            "eight rapini , mashed  ==>  { quantity: 8 , unit: count , item: rapini , mod: mashed }\n"
          ]
        }
      ],
      "source": [
        "# run some examples\n",
        "N_TRAINING = 100_000\n",
        "\n",
        "x, y = generate_data(N_TRAINING)\n",
        "\n",
        "for i in range(15):\n",
        "    print(f\"{x[i]}  ==>  {y[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5lX1XAoOlsD",
        "outputId": "00b2c90f-c0a8-4bbc-ec04-fa5f352a4321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7336 unique words used for training\n",
            "Longest number of words in input or output: 24\n",
            "['', '[UNK]', ',', 'dozen', 'a', 'two', 'three', '1/2', '2/3', 'third', 'half', 'one', \"baker's\", '3', 'six', '1', '3/4', 'ten', '4', '5', '7', '10', 'five', '8', '1/4']\n",
            "\n",
            "['', '[UNK]', ',', '}', '{', 'unit:', 'quantity:', 'mod:', 'item:', 'SOS', 'EOS', 'None', 'count', '12', '0.5', '1', '10', '0.333', '5', '6', '0.25', '8', '2', '4', '9']\n"
          ]
        }
      ],
      "source": [
        "# seq2seq model in this case\n",
        "# word tokenization\n",
        "print(f\"{len(ingredients) + len(mods) + len(units) + len(quantities)} unique words used for training\")\n",
        "# round up to 7400\n",
        "VOCAB_SIZE = 7400\n",
        "\n",
        "max_len = -1\n",
        "for i in range(N_TRAINING):\n",
        "    x_len = len(x[i].split())\n",
        "    y_len = len(y[i].split())\n",
        "    if x_len > max_len:\n",
        "        max_len = x_len\n",
        "    if y_len > max_len:\n",
        "        max_len = y_len\n",
        "\n",
        "print(f\"Longest number of words in input or output: {max_len}\")\n",
        "MAX_LENGTH = 30 # round up to be safe\n",
        "\n",
        "# text vectorization using keras functions\n",
        "# output_sequence_length are all same length by adding padding\n",
        "tv_layer_x = keras.layers.TextVectorization(VOCAB_SIZE, standardize=None, split='whitespace', output_sequence_length=MAX_LENGTH)\n",
        "tv_layer_y = keras.layers.TextVectorization(VOCAB_SIZE, standardize=None, split='whitespace', output_sequence_length=MAX_LENGTH)\n",
        "\n",
        "# use on training data\n",
        "tv_layer_x.adapt(x)\n",
        "# add start token and end token to data\n",
        "tv_layer_y.adapt([f\"SOS {seq} EOS\" for seq in y])\n",
        "\n",
        "print(tv_layer_x.get_vocabulary()[:25])\n",
        "print(\"\")\n",
        "print(tv_layer_y.get_vocabulary()[:25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wuywW0xOn7Y",
        "outputId": "294e0bda-528f-4bac-8255-f6936c53ee17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder input train shape: (90000,)\n",
            "decoder input train shape: (90000,)\n",
            "decoder targets train shape: (90000, 30)\n",
            "encoder input train example: b'10 quickcooking grits'\n",
            "decoder input train example: b'SOS { quantity: 10 , unit: count , item: quickcooking grits , mod: None }'\n",
            "decoder target train example: [   4    6   16    2    5   12    2    8 3698  662    2    7   11    3\n",
            "   10    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "# setting up inputs for model\n",
        "train_size = int(0.9 * N_TRAINING)\n",
        "\n",
        "encoder_input_train = tf.convert_to_tensor(x[:train_size])\n",
        "encoder_input_val = tf.convert_to_tensor(x[train_size:])\n",
        "\n",
        "# decoder input is what we want it to predict at the end of that timestep\n",
        "decoder_input_train = tf.convert_to_tensor([f\"SOS {seq}\" for seq in y[:train_size]])\n",
        "decoder_input_val = tf.convert_to_tensor([f\"SOS {seq}\" for seq in y[train_size:]])\n",
        "\n",
        "# use layer since need integer tokens instead of string\n",
        "decoder_targets_train = tv_layer_y([f\"{seq} EOS\" for seq in y[:train_size]])\n",
        "decoder_targets_val = tv_layer_y([f\"{seq} EOS\" for seq in y[train_size:]])\n",
        "\n",
        "print(f\"encoder input train shape: {encoder_input_train.shape}\")\n",
        "print(f\"decoder input train shape: {decoder_input_train.shape}\")\n",
        "print(f\"decoder targets train shape: {decoder_targets_train.shape}\")\n",
        "\n",
        "print(f\"encoder input train example: {encoder_input_train[0].numpy()}\")\n",
        "print(f\"decoder input train example: {decoder_input_train[0].numpy()}\")\n",
        "print(f\"decoder target train example: {decoder_targets_train[0].numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pJMZyZChO1Uq"
      },
      "outputs": [],
      "source": [
        "# MODEL BACKGROUND INFO by simon\n",
        "\n",
        "  # recurrent neural network\n",
        "    # different amounts of input data\n",
        "    # feedback loop allows using sequencial data (past data)\n",
        "    # input is both current and previous data\n",
        "    # problem: long sequences cause vanishing gradient descent (too long causes weights to become super big or small)\n",
        "\n",
        "  # LSTM (long short term memory)\n",
        "    # avoids exploding/vanishing gradient problem\n",
        "      # instead of using the same feedback loop, split into long term and short term memories\n",
        "    # forget gate: short term memory output determines what percent of long term memory is remembered (sigmoid)\n",
        "    # new long term memory is input to determine new short term memory to pass on\n",
        "\n",
        "  # word embedding and word2vec\n",
        "    # neural network assigns numbers to words based on context\n",
        "      #better than randomly assign as similar words have similar embeddings\n",
        "    # to include more context\n",
        "      # continuous bag of words: uses surrounding words to predict what goes in the middle\n",
        "      # skip gram: use middle word to predict surrounding words\n",
        "    # negative sampling: randomly selecting subset of words to not predict, optimizes training by reducing number of words\n",
        "\n",
        "   #seq2seq (uses all of the above)\n",
        "    # encoder: turns input into collection of long and short term memories (cell and hidden states)\n",
        "      # embedding layer to tokenize phrase into words and their embeddings\n",
        "      # layers of LSTM\n",
        "      # last cell and hidden states are called context vector\n",
        "    # decoder:\n",
        "      # input is context vector, decodes into output sentence\n",
        "      # new sets of LSTM\n",
        "      # lead to fully connected layer (basic neural network)\n",
        "        # softmax function picks output words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzZfwHNSOqRN",
        "outputId": "fa80b4a7-d994-43f6-8684-75c2a181283a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder input id shape: (None, 30)\n",
            "encoder input id type: <dtype: 'int64'>\n",
            "encoder embeddings shape: (None, 30, 96)\n",
            "decoder embeddings shape: (None, 30, 96)\n"
          ]
        }
      ],
      "source": [
        "# setting up seq2seq model\n",
        "# turn strings into numbers and add dimensionality for better representation as input to computer\n",
        "encoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
        "\n",
        "EMBED_SIZE = 96\n",
        "\n",
        "encoder_input_ids = tv_layer_x(encoder_inputs)\n",
        "decoder_input_ids = tv_layer_y(decoder_inputs)\n",
        "\n",
        "print(f\"encoder input id shape: {encoder_input_ids.shape}\")\n",
        "print(f\"encoder input id type: {encoder_input_ids.dtype}\")\n",
        "\n",
        "# input dim. x output dim. ==> (7000, 64)\n",
        "encoder_embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE, mask_zero=True)\n",
        "decoder_embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE, mask_zero=True)\n",
        "\n",
        "# keras functional api\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
        "\n",
        "print(f\"encoder embeddings shape: {encoder_embeddings.shape}\")\n",
        "print(f\"decoder embeddings shape: {decoder_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7WBqqoyfOrqz"
      },
      "outputs": [],
      "source": [
        "# set up encoder\n",
        "encoder = keras.layers.Bidirectional(keras.layers.LSTM(256, return_state=True))\n",
        "\n",
        "# discard outputs, keep cell and hidden states\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "\n",
        "# short term memory is 0 and 2, long term memory is 1 and 3\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1), tf.concat(encoder_state[1::2], axis=-1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nBPs9XSQOu77"
      },
      "outputs": [],
      "source": [
        "# set up decoder\n",
        "# should be twice the size as encoder as bidirectional is concatenated (forward + backward = 2 * forward)\n",
        "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
        "\n",
        "# decoder input is encoder states\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zBqiS_EpOwQA"
      },
      "outputs": [],
      "source": [
        "# set up dense layer to make predictions\n",
        "# softmax gives probability of all words, choose the highest as next word\n",
        "output_layer = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
        "Y_probabilities = output_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FufXiDYO615",
        "outputId": "3955eabc-97bc-441c-d67c-0b14126278fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " text_vectorization (TextVe  (None, 30)                   0         ['input_1[0][0]']             \n",
            " ctorization)                                                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 30, 96)               710400    ['text_vectorization[0][0]']  \n",
            "                                                                                                  \n",
            " text_vectorization_1 (Text  (None, 30)                   0         ['input_2[0][0]']             \n",
            " Vectorization)                                                                                   \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  [(None, 512),                722944    ['embedding[0][0]']           \n",
            " al)                          (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 30, 96)               710400    ['text_vectorization_1[0][0]']\n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)      (None, 512)                  0         ['bidirectional[0][1]',       \n",
            "                                                                     'bidirectional[0][3]']       \n",
            "                                                                                                  \n",
            " tf.concat_1 (TFOpLambda)    (None, 512)                  0         ['bidirectional[0][2]',       \n",
            "                                                                     'bidirectional[0][4]']       \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 30, 512)              1247232   ['embedding_1[0][0]',         \n",
            "                                                                     'tf.concat[0][0]',           \n",
            "                                                                     'tf.concat_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 30, 7400)             3796200   ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7187176 (27.42 MB)\n",
            "Trainable params: 7187176 (27.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# compile model\n",
        "model = keras.Model(inputs=[encoder_inputs,\n",
        "                    decoder_inputs],\n",
        "                    outputs=[Y_probabilities])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",  # sparse because each word token gets its own number (not one-hot)\n",
        "              optimizer=\"nadam\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_EBdHsClPB60"
      },
      "outputs": [],
      "source": [
        "# turns the model.summary() into a picture\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# plot_model(model, to_file = 'model.png', show_shapes = True, show_dtype = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thd5iaeTPDq_",
        "outputId": "d2a463aa-9d60-4304-b343-cdf469734939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n",
            "2813/2813 [==============================] - 142s 44ms/step - loss: 1.3691 - accuracy: 0.7504 - val_loss: 0.8095 - val_accuracy: 0.8296\n",
            "Epoch 2/11\n",
            "2813/2813 [==============================] - 103s 37ms/step - loss: 0.5373 - accuracy: 0.8823 - val_loss: 0.3234 - val_accuracy: 0.9266\n",
            "Epoch 3/11\n",
            "2813/2813 [==============================] - 102s 36ms/step - loss: 0.2062 - accuracy: 0.9512 - val_loss: 0.1377 - val_accuracy: 0.9664\n",
            "Epoch 4/11\n",
            "2813/2813 [==============================] - 101s 36ms/step - loss: 0.0980 - accuracy: 0.9747 - val_loss: 0.0798 - val_accuracy: 0.9781\n",
            "Epoch 5/11\n",
            "2813/2813 [==============================] - 100s 36ms/step - loss: 0.0575 - accuracy: 0.9831 - val_loss: 0.0479 - val_accuracy: 0.9856\n",
            "Epoch 6/11\n",
            "2813/2813 [==============================] - 102s 36ms/step - loss: 0.0275 - accuracy: 0.9928 - val_loss: 0.0211 - val_accuracy: 0.9948\n",
            "Epoch 7/11\n",
            "2813/2813 [==============================] - 101s 36ms/step - loss: 0.0134 - accuracy: 0.9971 - val_loss: 0.0129 - val_accuracy: 0.9971\n",
            "Epoch 8/11\n",
            "2813/2813 [==============================] - 99s 35ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 0.0131 - val_accuracy: 0.9969\n",
            "Epoch 9/11\n",
            "2813/2813 [==============================] - 100s 36ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0095 - val_accuracy: 0.9976\n",
            "Epoch 10/11\n",
            "2813/2813 [==============================] - 102s 36ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0070 - val_accuracy: 0.9981\n",
            "Epoch 11/11\n",
            "2813/2813 [==============================] - 103s 37ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0066 - val_accuracy: 0.9985\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7886a020f250>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EPOCHS = 11\n",
        "model.fit((encoder_input_train, decoder_input_train),\n",
        "          decoder_targets_train,\n",
        "          epochs = EPOCHS,\n",
        "          validation_data = ((encoder_input_val, decoder_input_val), decoder_targets_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "P7Z149EPdZJE"
      },
      "outputs": [],
      "source": [
        "model.save(\"ingredientsmodel.keras\", save_format=\"keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fYog9IENrWeB"
      },
      "outputs": [],
      "source": [
        "testmodel = tf.keras.models.load_model(\"ingredientsmodel.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29fiLjkLkcuz"
      },
      "outputs": [],
      "source": [
        "#model.export(\"test.tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxsGcig1kgqw"
      },
      "outputs": [],
      "source": [
        "#testmodel = tf.saved_model.load(\"test.tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wTIDxfLbPFKc"
      },
      "outputs": [],
      "source": [
        "# ingredient string to json formatted dictionary representation\n",
        "def convert(ingredient):\n",
        "  # translation is blank at first\n",
        "  translation = \"\"\n",
        "  for word_id in range(MAX_LENGTH):\n",
        "    # string to be translated on encoder input\n",
        "    x_enc = np.array([ingredient])\n",
        "    # string to be translated on decoder input\n",
        "    x_dec = np.array([\"SOS \" + translation])\n",
        "    # softmax probability distribution for predicted next word\n",
        "    y_probs = testmodel.predict((x_enc, x_dec), verbose = 0)[0, word_id]\n",
        "    # choose highest probability word\n",
        "    predicted_id = np.argmax(y_probs)\n",
        "    # find word based on id\n",
        "    predicted_word = tv_layer_y.get_vocabulary()[predicted_id]\n",
        "    # if word is EOS, means reached max length or end\n",
        "    if predicted_word == \"EOS\":\n",
        "      break\n",
        "    # else, add to prediction sequence and loop again for next word\n",
        "    translation += \" \" + predicted_word\n",
        "  return translation.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldzgzeFqjjSS",
        "outputId": "c56dd769-8ee2-41b2-ce43-40f799df541e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ quantity: 2 , unit: count , item: cucumbers , mod: None }\n",
            "{ quantity: 2 , unit: tablespoons , item: olive oil , mod: None }\n",
            "{ quantity: 3 , unit: cups , item: recipe mix , mod: None }\n",
            "{ quantity: 2 , unit: count , item: chicken drumsticks , mod: sliced }\n"
          ]
        }
      ],
      "source": [
        "print(convert(\"2 cucumbers\"))\n",
        "print(convert(\"2 tablespoons olive oil\"))\n",
        "print(convert(\"3 cups of rice\"))\n",
        "print(convert(\"2 sliced chicken breast\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w3i8R848cEP"
      },
      "outputs": [],
      "source": [
        "# IGNORE THE REST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WGuwKuUPPKK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Food Ingredients and Recipe Dataset with Image Name Mapping.csv\")\n",
        "df.head()\n",
        "# the images are in a separate folder\n",
        "# indexes should match to the recipes\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGbuVI_T3VbR"
      },
      "outputs": [],
      "source": [
        "print(convert(\"4 pounds jasmine rice\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C810HD8bPHJY"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "ingredients_column = []\n",
        "\n",
        "for recipe_ingredients in df['Ingredients'].items():\n",
        "  # list has quotes around it, need to convert from string to list\n",
        "  recipe_ingredients = ast.literal_eval(recipe_ingredients[1])\n",
        "  # make a list of the ingredients for each recipe\n",
        "  ingredients_list = []\n",
        "  for ingredient in recipe_ingredients:\n",
        "    if len(ingredient) != 0:\n",
        "      print(ingredient)\n",
        "      standardized = convert(ingredient)\n",
        "      print(standardized)\n",
        "      if standardized.find(\"item: \") > -1:\n",
        "        if (standardized.find(\", mod: \")) > -1:\n",
        "          standardized = standardized[standardized.index(\"item: \") + 6 : standardized.index(\" , mod: \")]\n",
        "        else:\n",
        "          standardized = standardized[standardized.index(\"item: \") + 6]\n",
        "        ingredients_list.append(standardized.strip())\n",
        "  ingredients_column.append({'Recipe Ingredients': ingredients_list})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afCVTMqhYeHm"
      },
      "outputs": [],
      "source": [
        "ingredients = pd.DataFrame(ingredients_column)\n",
        "ingredients.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXdQ_x77W6QM"
      },
      "outputs": [],
      "source": [
        "df.to_csv('standardized_ingredients.csv')\n",
        "from google.colab import files\n",
        "files.download(\"standardized_ingredients.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
